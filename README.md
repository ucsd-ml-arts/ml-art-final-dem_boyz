# Final Project

Mikhail Kardash, mkardash@ucsd.edu

Sean Liu, sel118@ucsd.edu

## Abstract Proposal

Does music make the video, or is the visual medium not influenced much by a musical one? In other words, can we treat music as a latent space for a music video? Our final project attempts to answer these questions by converting music videos downloaded from Youtube and taking spectrograms of 5 seconds of audio to generate video frames based on those spectrogram input images.  The final goal of this project is to create a music video with the images created.  

## Project Report

Upload your project report (4 pages) as a pdf with your repository, following this template: [google docs](https://docs.google.com/document/d/133H59WZBmH6MlAgFSskFLMQITeIC5d9b2iuzsOfa4E8/edit?usp=sharing).

File: Music_Video_Generation_using_Pix_to_Pix.pdf

## Model/Data

Briefly describe the files that are included with your repository:
- trained models
- training data (or link to training data)

## Code

Your code for generating your project:
- Jupyter notebooks: ECE_188_Test_Code.ipynb

## Results

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- What you include here will very much depend on the format of your final project
  - image files (`.jpg`, `.png` or whatever else is appropriate)
  - 3d models
  - movie files (uploaded to youtube or vimeo due to github file size limits)
  - audio files
  - ... some other form
  Movie.mp4, Movie200.mp4 represent produced results (see paper); gifs of images generated are also included

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference
[1] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros “Image-to-Image Translation with Conditional Adversarial Networks” 26 Nov 2018
